{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j35x6zPVU0En",
        "outputId": "45c728ca-b38d-4fcd-f9e9-637decaaca3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK5nq3oe6DZM",
        "outputId": "1b76d283-8317-4a17-ef4c-db05aa128dc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI1dvQcqWIA5",
        "outputId": "4843c235-25b8-4e74-87f2-6fb144ae14e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FR-FebbaWQJp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric\n",
        "import numpy as np\n",
        "from datasets import Features, Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "2gSMZz_AWpgD",
        "outputId": "e167f627-d33a-4d1c-8948-007a1a382155"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Charangan/MedBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset info:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['subject_id', 'hadm_id', 'admittime', 'dischtime', 'deathtime', 'admission_type', 'admit_provider_id', 'admission_location', 'discharge_location', 'insurance', 'language', 'marital_status', 'race', 'edregtime', 'edouttime', 'hospital_expire_flag'],\n",
            "        num_rows: 275\n",
            "    })\n",
            "})\n",
            "\n",
            "Dataset columns: ['subject_id', 'hadm_id', 'admittime', 'dischtime', 'deathtime', 'admission_type', 'admit_provider_id', 'admission_location', 'discharge_location', 'insurance', 'language', 'marital_status', 'race', 'edregtime', 'edouttime', 'hospital_expire_flag']\n",
            "\n",
            "Sample from tokenized dataset:\n",
            "{'input_ids': [101, 158, 2069, 16523, 15681, 157, 9664, 12412, 2271, 9637, 143, 21564, 2107, 145, 9025, 23203, 9159, 2162, 17447, 17656, 17516, 2137, 151, 19556, 13882, 11780, 6820, 19747, 2162, 12150, 3663, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0}\n",
            "\n",
            "Dataset features:\n",
            "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Value(dtype='int64', id=None)}\n",
            "\n",
            "Sample batch:\n",
            "input_ids: shape torch.Size([4, 275]), dtype torch.int64\n",
            "token_type_ids: shape torch.Size([4, 275]), dtype torch.int64\n",
            "attention_mask: shape torch.Size([4, 275]), dtype torch.int64\n",
            "labels: shape torch.Size([4]), dtype torch.int64\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='102' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [102/207 21:44 < 22:49, 0.08 it/s, Epoch 1.46/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='207' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [207/207 46:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 01:21]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 0.000129148960695602, 'eval_accuracy': 1.0, 'eval_runtime': 85.7589, 'eval_samples_per_second': 1.166, 'eval_steps_per_second': 0.292, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import Optional\n",
        "\n",
        "# Custom Trainer class to ensure contiguous tensors\n",
        "class CustomTrainer(Trainer):\n",
        "    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):\n",
        "        # Make all model parameters contiguous before saving\n",
        "        for param in self.model.parameters():\n",
        "            if not param.is_contiguous():\n",
        "                param.data = param.data.contiguous()\n",
        "\n",
        "        # Call the parent class's save_model method\n",
        "        super().save_model(output_dir, _internal_call)\n",
        "\n",
        "# Load pre-trained MedBERT model and tokenizer\n",
        "model_name = \"Charangan/MedBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set num_labels according to the number of classes in your readmission risk prediction task\n",
        "num_labels = 3  # For example: 'low', 'medium', 'high'\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# Define paths to datasets\n",
        "dataset_paths = {\n",
        "    'admissions': '/content/drive/MyDrive/dataset/admissions.csv',\n",
        "    'caregiver': '/content/drive/MyDrive/dataset/caregiver.csv',\n",
        "    'chartevents': '/content/drive/MyDrive/dataset/chartevents.csv',\n",
        "    'd_hcpcs': '/content/drive/MyDrive/dataset/d_hcpcs.csv',\n",
        "    'd_icd_diagnoses': '/content/drive/MyDrive/dataset/d_icd_diagnoses.csv',\n",
        "    'd_icd_procedures': '/content/drive/MyDrive/dataset/d_icd_procedures.csv',\n",
        "    'd_labitems': '/content/drive/MyDrive/dataset/d_labitems.csv',\n",
        "    'diagnoses_icd': '/content/drive/MyDrive/dataset/diagnoses_icd.csv',\n",
        "    'drgcodes': '/content/drive/MyDrive/dataset/drgcodes.csv',\n",
        "    'emar': '/content/drive/MyDrive/dataset/emar.csv',\n",
        "    'hcpcsevents': '/content/drive/MyDrive/dataset/hcpcsevents.csv',\n",
        "    'icustays': '/content/drive/MyDrive/dataset/icustays.csv',\n",
        "    'labevents': '/content/drive/MyDrive/dataset/labevents.csv',\n",
        "    'microbiologyevents': '/content/drive/MyDrive/dataset/microbiologyevents.csv',\n",
        "    'outputevents': '/content/drive/MyDrive/dataset/outputevents.csv',\n",
        "    'patients': '/content/drive/MyDrive/dataset/patients.csv',\n",
        "    'pharmacy': '/content/drive/MyDrive/dataset/pharmacy.csv',\n",
        "    'procedures_icd': '/content/drive/MyDrive/dataset/procedures_icd.csv',\n",
        "    'services': '/content/drive/MyDrive/dataset/services.csv',\n",
        "    'transfers': '/content/drive/MyDrive/dataset/transfers.csv',\n",
        "    'final_features':'/content/drive/MyDrive/dataset/finall_features.csv',\n",
        "    'target_variables_all_fin':'/content/drive/MyDrive/dataset/target_variables_all_fin.csv'\n",
        "}\n",
        "\n",
        "def tokenize_and_format(examples):\n",
        "    # Construct text from available fields\n",
        "    text = f\"{examples['admission_type']} {examples['admission_location']} {examples['discharge_location']}\"\n",
        "\n",
        "    # Tokenize\n",
        "    tokenized = tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=275,\n",
        "    )\n",
        "\n",
        "    # Add a dummy label (replace this with your actual label logic)\n",
        "    tokenized['labels'] = 0\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Load and prepare dataset\n",
        "dataset = load_dataset('csv', data_files={'train': dataset_paths['admissions']}, delimiter=',')\n",
        "\n",
        "# Print dataset info\n",
        "print(\"Dataset info:\")\n",
        "print(dataset)\n",
        "print(\"\\nDataset columns:\", dataset['train'].column_names)\n",
        "\n",
        "# Prepare the dataset\n",
        "tokenized_dataset = dataset['train'].map(tokenize_and_format, remove_columns=dataset['train'].column_names)\n",
        "\n",
        "# Print sample\n",
        "print(\"\\nSample from tokenized dataset:\")\n",
        "print(tokenized_dataset[0])\n",
        "\n",
        "# Verify dataset structure\n",
        "print(\"\\nDataset features:\")\n",
        "print(tokenized_dataset.features)\n",
        "\n",
        "# Split dataset\n",
        "train_dataset = tokenized_dataset.shuffle(seed=42).select(range(275))\n",
        "eval_dataset = tokenized_dataset.shuffle(seed=42).select(range(100))\n",
        "\n",
        "# Define compute_metrics function\n",
        "def compute_metrics(p):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    predictions = np.argmax(p.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=p.label_ids)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "# Initialize data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Initialize CustomTrainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Print sample batch\n",
        "print(\"\\nSample batch:\")\n",
        "batch = data_collator([train_dataset[i] for i in range(4)])\n",
        "for k, v in batch.items():\n",
        "    print(f\"{k}: shape {v.shape}, dtype {v.dtype}\")\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    trainer.train()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during training: {str(e)}\")\n",
        "    print(\"Input shape:\", batch['input_ids'].shape)\n",
        "    raise\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation results:\", results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mapping from numeric class labels to descriptive names\n",
        "class_labels = {\n",
        "    0: \"LOW\",\n",
        "    1: \"MEDIUM\",\n",
        "    2: \"HIGH\"\n",
        "}\n",
        "\n",
        "# Example sample input data formatted as a list of dictionaries\n",
        "sample_inputs = [\n",
        "    {\n",
        "        \"admission_type\": \"Emergency\",\n",
        "        \"admission_location\": \"ICU\",\n",
        "        \"discharge_location\": \"Home\",\n",
        "        \"age\": 70,\n",
        "        \"gender\": \"female\",\n",
        "        \"ethnicity\": \"Caucasian\",\n",
        "        \"marital_status\": \"single\",\n",
        "        \"employment_status\": \"unemployed\",\n",
        "        \"chronic_conditions\": [\"asthma\", \"heart disease\"],\n",
        "        \"previous_admissions\": 5,\n",
        "        \"length_of_stay\": 7,  # in days\n",
        "        \"primary_diagnosis\": \"chronic obstructive pulmonary disease\",\n",
        "        \"medications\": [\"albuterol\", \"lisinopril\"],\n",
        "        \"discharge_disposition\": \"home\",\n",
        "    }\n",
        "]\n",
        "\n",
        "# Tokenize the sample inputs\n",
        "def tokenize_sample(inputs):\n",
        "    # Construct text from available fields\n",
        "    texts = [f\"{item['admission_type']} {item['admission_location']} {item['discharge_location']} {item.get('age', '')} {item.get('gender', '')} {item.get('ethnicity', '')} {item.get('marital_status', '')} {item.get('employment_status', '')} {', '.join(item.get('chronic_conditions', []))} {item.get('previous_admissions', '')} {item.get('length_of_stay', '')} {item.get('primary_diagnosis', '')} {', '.join(item.get('medications', []))} {item.get('discharge_disposition', '')}\" for item in inputs]\n",
        "\n",
        "    # Tokenize\n",
        "    tokenized_samples = tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=275,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return tokenized_samples\n",
        "\n",
        "# Tokenize the sample inputs\n",
        "tokenized_samples = tokenize_sample(sample_inputs)\n",
        "\n",
        "# Move tensors to the same device as the model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "# Make predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    inputs = {k: v.to(device) for k, v in tokenized_samples.items()}\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# Print predictions with descriptive class names\n",
        "print(\"\\nPredictions for sample inputs:\")\n",
        "for i, sample in enumerate(sample_inputs):\n",
        "    predicted_class_index = predictions[i].item()\n",
        "    predicted_class_name = class_labels.get(predicted_class_index, \"Unknown\")\n",
        "    print(f\"Sample {i + 1}:\")\n",
        "    print(f\"  Input: {sample}\")\n",
        "    print(f\"  Predicted class: {predicted_class_name}\")\n"
      ],
      "metadata": {
        "id": "d9e4OEhZ3naw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93ce980-239f-41c6-b5b9-7490c1c9d4c2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predictions for sample inputs:\n",
            "Sample 1:\n",
            "  Input: {'admission_type': 'Emergency', 'admission_location': 'ICU', 'discharge_location': 'Home', 'age': 70, 'gender': 'female', 'ethnicity': 'Caucasian', 'marital_status': 'single', 'employment_status': 'unemployed', 'chronic_conditions': ['asthma', 'heart disease'], 'previous_admissions': 5, 'length_of_stay': 7, 'primary_diagnosis': 'chronic obstructive pulmonary disease', 'medications': ['albuterol', 'lisinopril'], 'discharge_disposition': 'home'}\n",
            "  Predicted class: MEDIUM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkGzQ7aucaD2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}